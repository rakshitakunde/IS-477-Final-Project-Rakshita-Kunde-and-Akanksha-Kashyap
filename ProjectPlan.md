Rakshita Kunde and Akanksha Kashyap IS 477 Project Plan

Overall Goal: The goal of this project is to design and implement an end-to-end data curation and analysis system that showcases the full data lifecycle, from acquiring the data to producing insightful results, of our two chosen datasets. We will be collecting, organizing, cleaning, and integrating the data using the appropriate tools and skills learned during lecture and the coursework. We will also be applying concepts, such as ethical data handling, data quality assessment, workflow automation, and reproducibility to show how we treat the datasets and arrive at our conclusions. We intend to use our data to show insights into real-world issues and events that showcase current gaps within society. 

Research Questions:
1. How does the time delay between a crime occurring and the public data update differ between the broad “Crimes - 2001 to Present” dataset and the specialized “Violence Reduction - Victims of Homicides and Non-Fatal Shootings” dataset?
2. Are the block-level locations in the "Crimes - 2001 to Present" dataset enough to prevent someone from identifying a specific victim when cross-referenced with the age, race, and sex fields in the "Violence Reduction - Victims of Homicides and Non-Fatal Shootings" dataset?
3. Do the City's Terms of Use allow for a merged dataset to be shared publicly, or are there restrictions on combining these two sources?
4. What specific unique fields in the "Violence Reduction - Victims of Homicides and Non-Fatal Shootings" dataset (like age or gender) require special handling or transformation during acquisition that are not a concern when acquiring the basic "Crimes - 2001 to Present" incident data?
5. What is the best way to design a database table structure that links the "Crimes - 2001 to Present" data (incident) to the "Violence Reduction - Victims of Homicides and Non-Fatal Shootings" data (person) without duplicating location and date information?
6. What’s the most reliable method for matching a victim record from the "Violence Reduction - Victims of Homicides and Non-Fatal Shootings" dataset back to its original incident record in the "Crimes - 2001 to Present” dataset, considering that an incident may have multiple victims?
7. Do a crime's final classification codes (i.e. IUCR codes) show inconsistencies between the "Crimes - 2001 to Present” dataset and the records in the "Violence Reduction - Victims of Homicides and Non-Fatal Shootings" dataset for the same event?
8. What’s the best strategy for handling missing values and correcting outliers across the two large, combined datasets?
9. How can an automated pipeline be built to ensure tracking and recording transformation steps from the original source files to the final, cleaned, and merged dataset?
10. What documentation and environment setup are necessary to make the entire process of cleaning and analyzing the merged data reproducible by an outside researcher?
11. What essential metadata must be created to ensure the integrated dataset is fully understandable and reusable by future analysts?

Team: Regarding our team member roles and responsibilities, both Rakshita Kunde and Akanksha Kashyap will work on all parts of the final project, splitting the responsibilities evenly. We will work on each part together to have effective collaboration in all aspects. To split the tasks evenly for the remainder of the project, Rakshita will be the data handler who will work with the Acquisition, Enrichment, Integration, and Reproducibility code and document the Data Profile, Reproducing Steps, and References. While, Akanksha will be the architecture lead handling the Storage Schema, Cleaning, and Automation and document the Lifecycle, Ethical Handling, Quality/Cleaning Summary, and Metadata.

Datasets: Datasets: The two datasets that we will be analyzing for our project is the City of Chicago “Crimes - 2001 to Present” (https://catalog.data.gov/dataset/crimes-2001-to-present) and the City of Chicago “Violence Reduction - Victims of Homicides and Non-Fatal Shootings” (https://catalog.data.gov/dataset/violence-reduction-victims-of-homicides-and-non-fatal-shootings). These two datasets were both published by the City of Chicago and derived from the Chicago Police Department (CPD) records. 
  The first dataset “Crimes - 2001 to Present” is a broader, more comprehensive record of all the reported crime incidents in Chicago from 2001 onward, extracted from the CPD’s CLEAR system. CLEAR stands for Citizen Law Enforcement Analysis and Reporting, which is an integrated, state-of-the-art information technology enterprise system. It serves as the central data warehouse and user interface for the CPD to support a wide range of police and criminal justice functions, whether that’s day-to-day policing, or in-depth crime analysis. 
  The second dataset “Violence Reduction - Victims of Homicides and Non-Fatal Shootings” is a more specialized dataset that focuses on individual-level victimizations related exclusively to homicides and non-fatal shootings. This dataset covers homicides that date back to 1991 and non-fatal shootings from 2010 to the present, and is updated daily with around a 48-hour lag.
  The “Crimes - 2001 to Present” dataset tracks incidents and includes different fields like the Illinois Uniform Crime Reporting (IUCR) code and block-level location, the “Violence Reduction - Victims of Homicides and Non-Fatal Shootings” dataset focuses on the individual victim, often referencing the IUCR codes from the “Crimes - 2001 to Present” dataset for classification. Overall, the "Violence Reduction - Victims of Homicides and Non-Fatal Shootings" dataset can be viewed as a specialized, violence-focused subset of the broader "Crimes - 2001 to Present" incident data, with both being updated daily, but with slightly different data lag policies.

Timeline: For the Team Selection and Project Plan milestones of this project, both members of the team worked evenly on both assignments to meet the deadlines and requirements necessary. To complete future milestones with the Interim Status Report and Final Project Submission, we have broken up the rest of the semester into 2 phases to ensure we get ample time to work with the data, meet our requirements, and complete the report thoroughly so it can be submitted by the given deadline. Phase 1 will consist of the brute work of working with the datasets themselves and better understanding both datasets so we can integrate them effectively. The second phase will focus on the finalization of the final report to ensure we communicate our findings sufficiently. The timeline below will show the deliverable, target date, who is working on it, and what requirements are completed. Here is our comprehensive timeline for the rest of the semester:

Phase 1:

1. Data Lifecycle and Ethical Handling, 10/14, Akanksha Kashyap, M1 & M2
2. Data Collection/Acquisition Scripts, 10/14, Rakshita Kunde, M3
3. Store & Organization Design, 10/14, Akanksha Kashyap, M4 & M5 
4. Extraction & Enrichment Scripts, 10/21, Rakshita Kunde, M6
5. Data Quality & Cleaning, 10/21, Akanksha Kashyap, M9 & M10
6. Data Integration Script, 10/21, Rakshita Kunde, M7 & M8
7. Workflow Automation Draft, 10/28, Akanksha Kashyap, M11 & M12
8. Reproducibility Setup Draft, 10/28, Rakshita Kunde, M13
9. Drafting Interim Status Report, 11/08, Rakshita Kunde & Akanksha Kashyap
10. Interim Status Report Submission, 11/11, Rakshita Kunde & Akanksha Kashyap, Interim Report & Git Release

Phase 2:
1. Final Workflow Implementation, 11/25, Akanksha Kashyap, M11 & M12
2. Final Reproducibility Implementation, 11/25, Rakshita Kunde, M13
3. Metadata & Documentation Finalization, 12/02, kanksha Kashyap, M15
4. Drafting Final Report, 12/09, Rakshita Kunde & Akanksha Kashyap
5. Final Project Submission, 12/10, Rakshita Kunde & Akanksha Kashyap, ALL REQUIREMENTS

Constraints: To create this project, there are several limitations and conditions that may affect how we collect, process, and analyze our datasets. First, the Crimes dataset covers incidents from 2001 onwards, however the Violence Reduction dataset includes victim-level records for homicides from 1991 to present and from 2010 onwards for shootings. The timeframes overlap but aren’t identical. Secondly, the crimes dataset records incidents while the Violence Reduction dataset records victims. Thirdly, both datasets are public, but they both involve sensitive information about crimes and victims. The data must be handled ethically in a way that avoid identification and stigmatization. Visualizations also must not expose individual-level locations without aggregation. Next, both datasets are updated daily and historical records may change upon new available information. Our interpretations may not be consistent over time. Additionally, the Crimes dataset has over 8 million rows, making it difficult to analyze all of the data. Finally, the two datasets use different naming conventions and have different data types and formatting. 

Gaps: While creating this project, we also need to take some gaps into consideration. Both datasets have missing or incomplete fields, which requires data cleaning strategies such as imputation or filtering. There is also a lack of contextual variables. Neither datasheet directly includes socioeconomic, demographic, or neighborhood-level information. This makes it difficult to interpret the data by location. There are also some ambiguous classifications. In the Crimes dataset, the offense type can be reclassified into Battery vs Aggravated Battery. In the Violence dataset, the Gunshot_Injury_I field may be “Unknown”. These categories need to be standardized to better interpret the data. There are no common keys between the two datasets, requiring us to join them by date, location, or offense type, leading to potential mismatches. Finally, both datasets use reported incidents, meaning that any unreported crimes are not documented in either dataset.
