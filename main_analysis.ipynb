{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_dir = \"data/final\"\n",
    "merged_path = os.path.join(final_data_dir, \"merged_victims_and_crimes.csv\")\n",
    "\n",
    "results_dir = \"results\"\n",
    "lag_analysis_path = os.path.join(results_dir, \"lag_analysis.txt\")\n",
    "iucr_analysis_path = os.path.join(results_dir, \"iucr_inconsistency_analysis.txt\")\n",
    "ethical_agg_analysis_path = os.path.join(results_dir, \"ethical_aggregate_analysis.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Analysis Functions ---\n",
    "\n",
    "These are 3 functions that help us to analyze specific parts of our research questions to best understand the datasets and its results :\n",
    "- **analyze_time_lag:** Compares the 'updated' lag from the Victims dataset vs. the 'updated_on' lag from the Crimes dataset to ansnwer research question 1 (How does the time delay between a crime occurring and the public data update differ between the broad “Crimes - 2001 to Present” dataset and the specialized “Violence Reduction - Victims of Homicides and Non-Fatal Shootings” dataset?)\n",
    "- **analyze_iucr_inconsistencies:** Identifies inconsistencies between the iucr variables in both datasets to answer research question 7 (Do a crime's final classification codes (i.e. IUCR codes) show inconsistencies between the \"Crimes - 2001 to Present” dataset and the records in the \"Violence Reduction - Victims of Homicides and Non-Fatal Shootings\" dataset for the same event?)\n",
    "- **analyze_generalized_data:** Performs an aggregate analysis on the generalized (safe) fields of the data to ensure it has been ethically handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_lag(df):\n",
    "\n",
    "    print(\"Running Time Lag Analysis...\")\n",
    "    \n",
    "    # Convert all date columns \n",
    "    df['date_victim'] = pd.to_datetime(df['date_victim'], errors='coerce')\n",
    "    df['updated'] = pd.to_datetime(df['updated'], errors='coerce')\n",
    "    \n",
    "    df['date_incident'] = pd.to_datetime(df['date_incident'], errors='coerce')\n",
    "    df['updated_on'] = pd.to_datetime(df['updated_on'], errors='coerce')\n",
    "\n",
    "    # Calculate lags in days\n",
    "    df['victim_data_lag'] = (df['updated'] - df['date_victim']).dt.days\n",
    "    df['incident_data_lag'] = (df['updated_on'] - df['date_incident']).dt.days\n",
    "    \n",
    "    # Get descriptive statistics for each lag\n",
    "    victim_lag_stats = df['victim_data_lag'].describe()\n",
    "    incident_lag_stats = df['incident_data_lag'].describe()\n",
    "\n",
    "    # 4. Save results to a text file\n",
    "    with open(lag_analysis_path, 'w') as f:\n",
    "        f.write(\"--- Time Lag Analysis ---\\n\\n\")\n",
    "        \n",
    "        f.write(\"Analysis of 'Violence Reduction - Victims' Dataset Lag:\\n\")\n",
    "        f.write(\"(Lag = 'updated' date - 'date_victim')\\n\")\n",
    "        f.write(str(victim_lag_stats))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Analysis of 'Crimes - 2001 to Present' Dataset Lag:\\n\")\n",
    "        f.write(\"(Lag = 'updated_on' date - 'date_incident')\\n\")\n",
    "        f.write(str(incident_lag_stats))\n",
    "    \n",
    "    print(f\"Results saved to '{lag_analysis_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_iucr_inconsistencies(df):\n",
    "    print(\"IUCR Inconsistency Analysis...\")\n",
    "    \n",
    "    # Identify the two columns to compare\n",
    "    victim_col = 'victimization_iucr_cd'\n",
    "    incident_col = 'iucr'\n",
    "    \n",
    "    # Find all rows where they are inconsistent (and both are non-null)\n",
    "    mismatches_df = df[\n",
    "        df[victim_col].notna() &\n",
    "        df[incident_col].notna() &\n",
    "        (df[victim_col] != df[incident_col])\n",
    "    ]\n",
    "    \n",
    "    num_mismatches = len(mismatches_df)\n",
    "    total_comparable = len(df[df[victim_col].notna() & df[incident_col].notna()])\n",
    "\n",
    "    # 3. Find the most common mismatch pairs\n",
    "    mismatch_pairs = mismatches_df.groupby([victim_col, incident_col]).size().reset_index(name='count')\n",
    "    mismatch_pairs = mismatch_pairs.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # 4. Save results to a text file\n",
    "    with open(iucr_analysis_path, 'w') as f:\n",
    "        f.write(\"--- IUCR Code Inconsistency Analysis ---\\n\\n\")\n",
    "        f.write(f\"Comparing: '{victim_col}' (from Victims) vs. '{incident_col}' (from Crimes)\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Total rows with comparable IUCR codes: {total_comparable}\\n\")\n",
    "        f.write(f\"Total rows with mismatched IUCR codes: {num_mismatches}\\n\")\n",
    "        f.write(f\"Percentage of mismatch: {(num_mismatches / total_comparable) * 100:.2f}%\\n\\n\")\n",
    "        \n",
    "        f.write(\"--- Top 20 Most Common Mismatches ---\\n\")\n",
    "        f.write(f\"({'Victim IUCR':<15} | {'Incident IUCR':<15} | {'Count':<10})\\n\")\n",
    "        f.write(\"-\" * 45 + \"\\n\")\n",
    "        for index, row in mismatch_pairs.head(20).iterrows():\n",
    "            f.write(f\"({row[victim_col]:<15} | {row[incident_col]:<15} | {row['count']:<10})\\n\")\n",
    "\n",
    "    print(f\"Results saved to '{iucr_analysis_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generalized_data(df):\n",
    "\n",
    "    print(\"Ethical Aggregate Analysis...\")\n",
    "    \n",
    "    # Define the generalized fields fixed in clean.ipynb\n",
    "    generalized_cols = ['community_area_victim', 'age_group', 'sex']\n",
    "\n",
    "    # Check if the columns exist\n",
    "    missing_cols = [col for col in generalized_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"ERROR: Missing required generalized columns: {missing_cols}\")\n",
    "        print(\"This analysis uses the 'safe' columns created in clean.ipynb.\")\n",
    "        return\n",
    "\n",
    "    # Run a simple aggregate group-by\n",
    "    agg_counts = df.groupby(generalized_cols).size().reset_index(name='victim_count')\n",
    "    agg_counts = agg_counts.sort_values(by='victim_count', ascending=False)\n",
    "    \n",
    "    # Save the results\n",
    "    with open(ethical_agg_analysis_path, 'w') as f:\n",
    "        f.write(\"--- Ethical Aggregate Analysis Report ---\\n\\n\")\n",
    "        f.write(\"This analysis demonstrates the use of generalized fields ('age_group', 'community_area')\\n\")\n",
    "        f.write(\"to perform analysis without using high-risk individual identifiers.\\n\\n\")\n",
    "        f.write(\"--- Top 20 Most Frequent Victim Groups (Aggregated) ---\\n\")\n",
    "        f.write(agg_counts.head(20).to_string())\n",
    "\n",
    "    print(f\"The results were saved to '{ethical_agg_analysis_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ Main Analysis Function ------\n",
    "\n",
    "This is the main script execution that loads the final merged data from `merged_victims_and_crimes.cv` and runs all the declared analysis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"--- Starting Data Analysis ---\")\n",
    "    \n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Loading merged dataset from '{merged_path}'...\")\n",
    "    try:\n",
    "        merged_df = pd.read_csv(merged_path, low_memory=False)\n",
    "        print(f\"The merged dataloaded ({len(merged_df)} rows).\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: The merged data file was not found. Run 'integrate.ipynb' first.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading the merged data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Run all analyses\n",
    "    analyze_time_lag(merged_df.copy())\n",
    "    analyze_iucr_inconsistencies(merged_df.copy())\n",
    "    analyze_generalized_data(merged_df.copy())\n",
    "    \n",
    "    print(\"\\n--- Data Analysis Complete ---\")\n",
    "    print(f\"All the results were saved in '{results_dir}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Analysis ---\n",
      "Loading merged dataset from 'data/final/merged_victims_and_crimes.csv'...\n",
      "The merged dataloaded (64536 rows).\n",
      "Running Time Lag Analysis...\n",
      "Results saved to 'results/lag_analysis.txt'\n",
      "IUCR Inconsistency Analysis...\n",
      "Results saved to 'results/iucr_inconsistency_analysis.txt'\n",
      "Ethical Aggregate Analysis...\n",
      "The results were saved to 'results/ethical_aggregate_analysis.txt'\n",
      "\n",
      "--- Data Analysis Complete ---\n",
      "All the results were saved in 'results' folder.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
